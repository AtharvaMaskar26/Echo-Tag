{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import librosa\n",
    "import librosa.display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<librosa.display.AdaptiveWaveplot at 0x18fb5fadcd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILENAME = \"bark.wav\"\n",
    "\n",
    "librosa_audio_data, librosa_sample_rate = librosa.load(FILENAME)\n",
    "librosa.display.waveshow(librosa_audio_data, sr=librosa_sample_rate, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>99812-1-2-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>159.522205</td>\n",
       "      <td>163.522205</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>99812-1-3-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>181.142431</td>\n",
       "      <td>183.284976</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>99812-1-4-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>242.691902</td>\n",
       "      <td>246.197885</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>99812-1-5-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>253.209850</td>\n",
       "      <td>255.741948</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>99812-1-6-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>332.289233</td>\n",
       "      <td>334.821332</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8732 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         slice_file_name    fsID       start         end  salience  fold  \\\n",
       "0       100032-3-0-0.wav  100032    0.000000    0.317551         1     5   \n",
       "1     100263-2-0-117.wav  100263   58.500000   62.500000         1     5   \n",
       "2     100263-2-0-121.wav  100263   60.500000   64.500000         1     5   \n",
       "3     100263-2-0-126.wav  100263   63.000000   67.000000         1     5   \n",
       "4     100263-2-0-137.wav  100263   68.500000   72.500000         1     5   \n",
       "...                  ...     ...         ...         ...       ...   ...   \n",
       "8727     99812-1-2-0.wav   99812  159.522205  163.522205         2     7   \n",
       "8728     99812-1-3-0.wav   99812  181.142431  183.284976         2     7   \n",
       "8729     99812-1-4-0.wav   99812  242.691902  246.197885         2     7   \n",
       "8730     99812-1-5-0.wav   99812  253.209850  255.741948         2     7   \n",
       "8731     99812-1-6-0.wav   99812  332.289233  334.821332         2     7   \n",
       "\n",
       "      classID             class  \n",
       "0           3          dog_bark  \n",
       "1           2  children_playing  \n",
       "2           2  children_playing  \n",
       "3           2  children_playing  \n",
       "4           2  children_playing  \n",
       "...       ...               ...  \n",
       "8727        1          car_horn  \n",
       "8728        1          car_horn  \n",
       "8729        1          car_horn  \n",
       "8730        1          car_horn  \n",
       "8731        1          car_horn  \n",
       "\n",
       "[8732 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the metadata \n",
    "\n",
    "metadata = pd.read_csv(\"UrbanSound8K/UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "dog_bark            1000\n",
       "children_playing    1000\n",
       "air_conditioner     1000\n",
       "street_music        1000\n",
       "engine_idling       1000\n",
       "jackhammer          1000\n",
       "drilling            1000\n",
       "siren                929\n",
       "car_horn             429\n",
       "gun_shot             374\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the dataset is balanced or not \n",
    "\n",
    "metadata[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features \n",
    "\n",
    "Here we will be using Mel-Frequency Cepstral Coefficients (MFCC) from the audio samples. The MFCC summarises the frequence distribution across the window size, so it is possible to analyze both the frequency and characterstics of the sound. These audio representations will allow us to identify features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 73)\n"
     ]
    }
   ],
   "source": [
    "mfccs = librosa.feature.mfcc(y = librosa_audio_data, sr = librosa_sample_rate, n_mfcc=40)\n",
    "\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.92297272e+02, -4.75054657e+02, -4.75254913e+02, ...,\n",
       "        -3.97302856e+02, -4.12189819e+02, -4.33255920e+02],\n",
       "       [ 7.39831848e+01,  7.58931046e+01,  7.31992188e+01, ...,\n",
       "         1.26724915e+02,  1.19925041e+02,  1.08599457e+02],\n",
       "       [ 2.51829643e+01,  1.96938095e+01,  1.80576859e+01, ...,\n",
       "        -5.74281812e-01, -1.63246110e-01,  5.04161978e+00],\n",
       "       ...,\n",
       "       [ 3.00478888e+00,  3.71383500e+00,  3.80284643e+00, ...,\n",
       "         4.47916985e-01,  7.36044109e-01,  4.10818243e+00],\n",
       "       [ 4.93726444e+00,  4.03950024e+00,  3.40298915e+00, ...,\n",
       "         2.87281728e+00,  2.34909701e+00,  2.91369009e+00],\n",
       "       [ 4.15801907e+00,  6.22881651e+00,  3.87448359e+00, ...,\n",
       "        -1.16615415e+00,  3.34697342e+00,  1.65836370e+00]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DATASET_PATH = 'UrbanSound8K/UrbanSound8K/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file): \n",
    "    audio, sample_rate = librosa.load(file_name, res_type = 'kaiser_fast')\n",
    "    mfccs_features = librosa.feature.mfcc(y = audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
    "\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3555it [03:30, 18.09it/s]c:\\Users\\athar\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8325it [07:59, 26.87it/s]c:\\Users\\athar\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "c:\\Users\\athar\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [08:19, 17.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(AUDIO_DATASET_PATH),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data, final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-217.35526, 70.22338, -130.38527, -53.282898,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-424.09818, 109.34077, -52.919525, 60.86475, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-458.79114, 121.38419, -46.52066, 52.00812, -...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-413.89984, 101.66373, -35.42945, 53.036354, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-446.60352, 113.68541, -52.402206, 60.302044,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-217.35526, 70.22338, -130.38527, -53.282898,...          dog_bark\n",
       "1  [-424.09818, 109.34077, -52.919525, 60.86475, ...  children_playing\n",
       "2  [-458.79114, 121.38419, -46.52066, 52.00812, -...  children_playing\n",
       "3  [-413.89984, 101.66373, -35.42945, 53.036354, ...  children_playing\n",
       "4  [-446.60352, 113.68541, -52.402206, 60.302044,...  children_playing"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df = pd.DataFrame(extracted_features, columns=['feature', 'class'])\n",
    "\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into dependent and independent features \n",
    "\n",
    "X = np.array(extracted_features_df['feature'].tolist())\n",
    "y = np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dog_bark', 'children_playing', 'children_playing', ...,\n",
       "       'car_horn', 'car_horn', 'car_horn'], dtype='<U16')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 40)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6985, 40)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1747, 40)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\athar\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1211 - loss: 26.4500\n",
      "Epoch 1: val_loss improved from inf to 2.29344, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1212 - loss: 25.6272 - val_accuracy: 0.1139 - val_loss: 2.2934\n",
      "Epoch 2/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1207 - loss: 2.7482\n",
      "Epoch 2: val_loss improved from 2.29344 to 2.28185, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1206 - loss: 2.7417 - val_accuracy: 0.1110 - val_loss: 2.2818\n",
      "Epoch 3/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1127 - loss: 2.3732\n",
      "Epoch 3: val_loss improved from 2.28185 to 2.27032, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1130 - loss: 2.3712 - val_accuracy: 0.1208 - val_loss: 2.2703\n",
      "Epoch 4/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1196 - loss: 2.3097\n",
      "Epoch 4: val_loss improved from 2.27032 to 2.24339, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1197 - loss: 2.3094 - val_accuracy: 0.1294 - val_loss: 2.2434\n",
      "Epoch 5/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1226 - loss: 2.2711\n",
      "Epoch 5: val_loss improved from 2.24339 to 2.21538, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1228 - loss: 2.2708 - val_accuracy: 0.1586 - val_loss: 2.2154\n",
      "Epoch 6/100\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1539 - loss: 2.2409\n",
      "Epoch 6: val_loss improved from 2.21538 to 2.14752, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1539 - loss: 2.2400 - val_accuracy: 0.1981 - val_loss: 2.1475\n",
      "Epoch 7/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1534 - loss: 2.2031\n",
      "Epoch 7: val_loss improved from 2.14752 to 2.10373, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1540 - loss: 2.2029 - val_accuracy: 0.1923 - val_loss: 2.1037\n",
      "Epoch 8/100\n",
      "\u001b[1m199/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1924 - loss: 2.1479\n",
      "Epoch 8: val_loss improved from 2.10373 to 2.03551, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1913 - loss: 2.1488 - val_accuracy: 0.2290 - val_loss: 2.0355\n",
      "Epoch 9/100\n",
      "\u001b[1m198/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1834 - loss: 2.1285\n",
      "Epoch 9: val_loss improved from 2.03551 to 2.00227, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1844 - loss: 2.1281 - val_accuracy: 0.2370 - val_loss: 2.0023\n",
      "Epoch 10/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2105 - loss: 2.0873\n",
      "Epoch 10: val_loss improved from 2.00227 to 1.97502, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2105 - loss: 2.0873 - val_accuracy: 0.2421 - val_loss: 1.9750\n",
      "Epoch 11/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2235 - loss: 2.0640\n",
      "Epoch 11: val_loss improved from 1.97502 to 1.96259, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2235 - loss: 2.0640 - val_accuracy: 0.2770 - val_loss: 1.9626\n",
      "Epoch 12/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2184 - loss: 2.0541\n",
      "Epoch 12: val_loss improved from 1.96259 to 1.94167, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2184 - loss: 2.0538 - val_accuracy: 0.2730 - val_loss: 1.9417\n",
      "Epoch 13/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2340 - loss: 2.0354\n",
      "Epoch 13: val_loss improved from 1.94167 to 1.91135, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2340 - loss: 2.0353 - val_accuracy: 0.2742 - val_loss: 1.9114\n",
      "Epoch 14/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2318 - loss: 2.0214\n",
      "Epoch 14: val_loss improved from 1.91135 to 1.87011, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2326 - loss: 2.0195 - val_accuracy: 0.3114 - val_loss: 1.8701\n",
      "Epoch 15/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2495 - loss: 1.9737\n",
      "Epoch 15: val_loss improved from 1.87011 to 1.85383, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2496 - loss: 1.9734 - val_accuracy: 0.3297 - val_loss: 1.8538\n",
      "Epoch 16/100\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2485 - loss: 1.9692\n",
      "Epoch 16: val_loss improved from 1.85383 to 1.81037, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2499 - loss: 1.9668 - val_accuracy: 0.3148 - val_loss: 1.8104\n",
      "Epoch 17/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2600 - loss: 1.9228\n",
      "Epoch 17: val_loss improved from 1.81037 to 1.77925, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2609 - loss: 1.9218 - val_accuracy: 0.3486 - val_loss: 1.7793\n",
      "Epoch 18/100\n",
      "\u001b[1m213/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2978 - loss: 1.8664\n",
      "Epoch 18: val_loss improved from 1.77925 to 1.71526, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2980 - loss: 1.8661 - val_accuracy: 0.3761 - val_loss: 1.7153\n",
      "Epoch 19/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3290 - loss: 1.8133\n",
      "Epoch 19: val_loss improved from 1.71526 to 1.65550, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3295 - loss: 1.8128 - val_accuracy: 0.3995 - val_loss: 1.6555\n",
      "Epoch 20/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3497 - loss: 1.7695\n",
      "Epoch 20: val_loss improved from 1.65550 to 1.60850, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3497 - loss: 1.7695 - val_accuracy: 0.4699 - val_loss: 1.6085\n",
      "Epoch 21/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3592 - loss: 1.7295\n",
      "Epoch 21: val_loss improved from 1.60850 to 1.52872, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3601 - loss: 1.7285 - val_accuracy: 0.4928 - val_loss: 1.5287\n",
      "Epoch 22/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3848 - loss: 1.6758\n",
      "Epoch 22: val_loss improved from 1.52872 to 1.47178, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3851 - loss: 1.6757 - val_accuracy: 0.5083 - val_loss: 1.4718\n",
      "Epoch 23/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4120 - loss: 1.6455\n",
      "Epoch 23: val_loss improved from 1.47178 to 1.42143, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4120 - loss: 1.6452 - val_accuracy: 0.5484 - val_loss: 1.4214\n",
      "Epoch 24/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 1.5809\n",
      "Epoch 24: val_loss improved from 1.42143 to 1.39436, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4318 - loss: 1.5810 - val_accuracy: 0.5409 - val_loss: 1.3944\n",
      "Epoch 25/100\n",
      "\u001b[1m192/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4328 - loss: 1.5871\n",
      "Epoch 25: val_loss improved from 1.39436 to 1.37500, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4347 - loss: 1.5820 - val_accuracy: 0.5570 - val_loss: 1.3750\n",
      "Epoch 26/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4554 - loss: 1.5367\n",
      "Epoch 26: val_loss improved from 1.37500 to 1.36020, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4554 - loss: 1.5368 - val_accuracy: 0.5650 - val_loss: 1.3602\n",
      "Epoch 27/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4550 - loss: 1.5274\n",
      "Epoch 27: val_loss improved from 1.36020 to 1.32004, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4552 - loss: 1.5271 - val_accuracy: 0.5724 - val_loss: 1.3200\n",
      "Epoch 28/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4662 - loss: 1.4800\n",
      "Epoch 28: val_loss improved from 1.32004 to 1.29396, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4672 - loss: 1.4789 - val_accuracy: 0.5816 - val_loss: 1.2940\n",
      "Epoch 29/100\n",
      "\u001b[1m196/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4910 - loss: 1.4431\n",
      "Epoch 29: val_loss improved from 1.29396 to 1.26047, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4918 - loss: 1.4433 - val_accuracy: 0.5856 - val_loss: 1.2605\n",
      "Epoch 30/100\n",
      "\u001b[1m200/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4953 - loss: 1.4100\n",
      "Epoch 30: val_loss improved from 1.26047 to 1.22710, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4963 - loss: 1.4103 - val_accuracy: 0.6153 - val_loss: 1.2271\n",
      "Epoch 31/100\n",
      "\u001b[1m193/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5197 - loss: 1.3991\n",
      "Epoch 31: val_loss improved from 1.22710 to 1.19579, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5191 - loss: 1.3994 - val_accuracy: 0.6188 - val_loss: 1.1958\n",
      "Epoch 32/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5127 - loss: 1.3847\n",
      "Epoch 32: val_loss did not improve from 1.19579\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5129 - loss: 1.3844 - val_accuracy: 0.6073 - val_loss: 1.1970\n",
      "Epoch 33/100\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5439 - loss: 1.3404\n",
      "Epoch 33: val_loss improved from 1.19579 to 1.18881, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5435 - loss: 1.3407 - val_accuracy: 0.6171 - val_loss: 1.1888\n",
      "Epoch 34/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 1.3569\n",
      "Epoch 34: val_loss improved from 1.18881 to 1.17580, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5317 - loss: 1.3565 - val_accuracy: 0.6148 - val_loss: 1.1758\n",
      "Epoch 35/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5479 - loss: 1.3157\n",
      "Epoch 35: val_loss improved from 1.17580 to 1.15893, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5476 - loss: 1.3158 - val_accuracy: 0.6211 - val_loss: 1.1589\n",
      "Epoch 36/100\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5664 - loss: 1.3163\n",
      "Epoch 36: val_loss improved from 1.15893 to 1.13137, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5650 - loss: 1.3166 - val_accuracy: 0.6440 - val_loss: 1.1314\n",
      "Epoch 37/100\n",
      "\u001b[1m199/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5533 - loss: 1.2956\n",
      "Epoch 37: val_loss did not improve from 1.13137\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5531 - loss: 1.2959 - val_accuracy: 0.6251 - val_loss: 1.1587\n",
      "Epoch 38/100\n",
      "\u001b[1m204/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5380 - loss: 1.3280\n",
      "Epoch 38: val_loss improved from 1.13137 to 1.10354, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5393 - loss: 1.3254 - val_accuracy: 0.6400 - val_loss: 1.1035\n",
      "Epoch 39/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5934 - loss: 1.2379\n",
      "Epoch 39: val_loss did not improve from 1.10354\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5925 - loss: 1.2396 - val_accuracy: 0.6359 - val_loss: 1.1043\n",
      "Epoch 40/100\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5641 - loss: 1.2743\n",
      "Epoch 40: val_loss improved from 1.10354 to 1.08041, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5641 - loss: 1.2742 - val_accuracy: 0.6400 - val_loss: 1.0804\n",
      "Epoch 41/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5624 - loss: 1.2507\n",
      "Epoch 41: val_loss improved from 1.08041 to 1.05528, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5626 - loss: 1.2504 - val_accuracy: 0.6525 - val_loss: 1.0553\n",
      "Epoch 42/100\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5904 - loss: 1.2242\n",
      "Epoch 42: val_loss did not improve from 1.05528\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5900 - loss: 1.2245 - val_accuracy: 0.6531 - val_loss: 1.0719\n",
      "Epoch 43/100\n",
      "\u001b[1m205/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5920 - loss: 1.2206\n",
      "Epoch 43: val_loss did not improve from 1.05528\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5920 - loss: 1.2206 - val_accuracy: 0.6566 - val_loss: 1.0580\n",
      "Epoch 44/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5723 - loss: 1.2454\n",
      "Epoch 44: val_loss improved from 1.05528 to 1.02952, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5725 - loss: 1.2450 - val_accuracy: 0.6651 - val_loss: 1.0295\n",
      "Epoch 45/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5920 - loss: 1.1873\n",
      "Epoch 45: val_loss did not improve from 1.02952\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5918 - loss: 1.1878 - val_accuracy: 0.6457 - val_loss: 1.0628\n",
      "Epoch 46/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5938 - loss: 1.2150\n",
      "Epoch 46: val_loss did not improve from 1.02952\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5938 - loss: 1.2149 - val_accuracy: 0.6754 - val_loss: 1.0312\n",
      "Epoch 47/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5969 - loss: 1.1739\n",
      "Epoch 47: val_loss improved from 1.02952 to 1.00303, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5970 - loss: 1.1739 - val_accuracy: 0.6766 - val_loss: 1.0030\n",
      "Epoch 48/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6156 - loss: 1.1591\n",
      "Epoch 48: val_loss improved from 1.00303 to 0.99393, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6155 - loss: 1.1591 - val_accuracy: 0.6680 - val_loss: 0.9939\n",
      "Epoch 49/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5959 - loss: 1.1912\n",
      "Epoch 49: val_loss improved from 0.99393 to 0.97910, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5958 - loss: 1.1909 - val_accuracy: 0.6875 - val_loss: 0.9791\n",
      "Epoch 50/100\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6170 - loss: 1.1440\n",
      "Epoch 50: val_loss did not improve from 0.97910\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6166 - loss: 1.1449 - val_accuracy: 0.6680 - val_loss: 0.9887\n",
      "Epoch 51/100\n",
      "\u001b[1m188/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6229 - loss: 1.1348\n",
      "Epoch 51: val_loss improved from 0.97910 to 0.96705, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 1.1371 - val_accuracy: 0.6760 - val_loss: 0.9671\n",
      "Epoch 52/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6278 - loss: 1.1246\n",
      "Epoch 52: val_loss improved from 0.96705 to 0.95667, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6272 - loss: 1.1252 - val_accuracy: 0.6846 - val_loss: 0.9567\n",
      "Epoch 53/100\n",
      "\u001b[1m196/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6171 - loss: 1.1200\n",
      "Epoch 53: val_loss did not improve from 0.95667\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6170 - loss: 1.1217 - val_accuracy: 0.6806 - val_loss: 0.9689\n",
      "Epoch 54/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6177 - loss: 1.1318\n",
      "Epoch 54: val_loss improved from 0.95667 to 0.93870, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6178 - loss: 1.1316 - val_accuracy: 0.6972 - val_loss: 0.9387\n",
      "Epoch 55/100\n",
      "\u001b[1m200/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6189 - loss: 1.1366\n",
      "Epoch 55: val_loss did not improve from 0.93870\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6190 - loss: 1.1348 - val_accuracy: 0.6915 - val_loss: 0.9641\n",
      "Epoch 56/100\n",
      "\u001b[1m189/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6145 - loss: 1.1419\n",
      "Epoch 56: val_loss improved from 0.93870 to 0.93379, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6154 - loss: 1.1376 - val_accuracy: 0.6903 - val_loss: 0.9338\n",
      "Epoch 57/100\n",
      "\u001b[1m206/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6364 - loss: 1.0937\n",
      "Epoch 57: val_loss did not improve from 0.93379\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6361 - loss: 1.0939 - val_accuracy: 0.6926 - val_loss: 0.9515\n",
      "Epoch 58/100\n",
      "\u001b[1m196/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6305 - loss: 1.1135\n",
      "Epoch 58: val_loss did not improve from 0.93379\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6293 - loss: 1.1153 - val_accuracy: 0.6857 - val_loss: 0.9654\n",
      "Epoch 59/100\n",
      "\u001b[1m202/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6255 - loss: 1.1064\n",
      "Epoch 59: val_loss did not improve from 0.93379\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6257 - loss: 1.1053 - val_accuracy: 0.6835 - val_loss: 0.9448\n",
      "Epoch 60/100\n",
      "\u001b[1m203/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6365 - loss: 1.0847\n",
      "Epoch 60: val_loss improved from 0.93379 to 0.91274, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6365 - loss: 1.0846 - val_accuracy: 0.7052 - val_loss: 0.9127\n",
      "Epoch 61/100\n",
      "\u001b[1m195/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6289 - loss: 1.1121\n",
      "Epoch 61: val_loss did not improve from 0.91274\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6290 - loss: 1.1107 - val_accuracy: 0.7046 - val_loss: 0.9205\n",
      "Epoch 62/100\n",
      "\u001b[1m215/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6402 - loss: 1.0754\n",
      "Epoch 62: val_loss did not improve from 0.91274\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6401 - loss: 1.0758 - val_accuracy: 0.6966 - val_loss: 0.9184\n",
      "Epoch 63/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6417 - loss: 1.0796\n",
      "Epoch 63: val_loss did not improve from 0.91274\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6416 - loss: 1.0797 - val_accuracy: 0.7046 - val_loss: 0.9139\n",
      "Epoch 64/100\n",
      "\u001b[1m188/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6322 - loss: 1.0563\n",
      "Epoch 64: val_loss improved from 0.91274 to 0.89734, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6329 - loss: 1.0580 - val_accuracy: 0.7207 - val_loss: 0.8973\n",
      "Epoch 65/100\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6530 - loss: 1.0286\n",
      "Epoch 65: val_loss did not improve from 0.89734\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6525 - loss: 1.0303 - val_accuracy: 0.6926 - val_loss: 0.9073\n",
      "Epoch 66/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6426 - loss: 1.0575\n",
      "Epoch 66: val_loss did not improve from 0.89734\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6426 - loss: 1.0577 - val_accuracy: 0.7127 - val_loss: 0.9006\n",
      "Epoch 67/100\n",
      "\u001b[1m191/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6457 - loss: 1.0232\n",
      "Epoch 67: val_loss did not improve from 0.89734\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6459 - loss: 1.0257 - val_accuracy: 0.7212 - val_loss: 0.8978\n",
      "Epoch 68/100\n",
      "\u001b[1m218/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6399 - loss: 1.0722\n",
      "Epoch 68: val_loss improved from 0.89734 to 0.88623, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6398 - loss: 1.0723 - val_accuracy: 0.7167 - val_loss: 0.8862\n",
      "Epoch 69/100\n",
      "\u001b[1m191/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6261 - loss: 1.0848\n",
      "Epoch 69: val_loss improved from 0.88623 to 0.88429, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6275 - loss: 1.0832 - val_accuracy: 0.7161 - val_loss: 0.8843\n",
      "Epoch 70/100\n",
      "\u001b[1m199/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6507 - loss: 1.0557\n",
      "Epoch 70: val_loss improved from 0.88429 to 0.87386, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6502 - loss: 1.0550 - val_accuracy: 0.7184 - val_loss: 0.8739\n",
      "Epoch 71/100\n",
      "\u001b[1m193/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6503 - loss: 1.0518\n",
      "Epoch 71: val_loss did not improve from 0.87386\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6503 - loss: 1.0505 - val_accuracy: 0.7218 - val_loss: 0.8759\n",
      "Epoch 72/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6528 - loss: 1.0469\n",
      "Epoch 72: val_loss improved from 0.87386 to 0.87277, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6523 - loss: 1.0478 - val_accuracy: 0.7310 - val_loss: 0.8728\n",
      "Epoch 73/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6304 - loss: 1.0556\n",
      "Epoch 73: val_loss did not improve from 0.87277\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6313 - loss: 1.0548 - val_accuracy: 0.7046 - val_loss: 0.8913\n",
      "Epoch 74/100\n",
      "\u001b[1m197/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6355 - loss: 1.0829\n",
      "Epoch 74: val_loss improved from 0.87277 to 0.87034, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6371 - loss: 1.0794 - val_accuracy: 0.7338 - val_loss: 0.8703\n",
      "Epoch 75/100\n",
      "\u001b[1m207/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6484 - loss: 1.0426\n",
      "Epoch 75: val_loss improved from 0.87034 to 0.86517, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6483 - loss: 1.0421 - val_accuracy: 0.7212 - val_loss: 0.8652\n",
      "Epoch 76/100\n",
      "\u001b[1m208/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6510 - loss: 1.0324\n",
      "Epoch 76: val_loss improved from 0.86517 to 0.86332, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6513 - loss: 1.0319 - val_accuracy: 0.7218 - val_loss: 0.8633\n",
      "Epoch 77/100\n",
      "\u001b[1m201/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6487 - loss: 1.0436\n",
      "Epoch 77: val_loss did not improve from 0.86332\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6488 - loss: 1.0437 - val_accuracy: 0.7315 - val_loss: 0.8713\n",
      "Epoch 78/100\n",
      "\u001b[1m190/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6584 - loss: 1.0443\n",
      "Epoch 78: val_loss did not improve from 0.86332\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6581 - loss: 1.0427 - val_accuracy: 0.7224 - val_loss: 0.8679\n",
      "Epoch 79/100\n",
      "\u001b[1m210/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6640 - loss: 0.9950\n",
      "Epoch 79: val_loss did not improve from 0.86332\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6635 - loss: 0.9960 - val_accuracy: 0.7304 - val_loss: 0.8722\n",
      "Epoch 80/100\n",
      "\u001b[1m191/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6474 - loss: 1.0254\n",
      "Epoch 80: val_loss improved from 0.86332 to 0.84053, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6477 - loss: 1.0262 - val_accuracy: 0.7275 - val_loss: 0.8405\n",
      "Epoch 81/100\n",
      "\u001b[1m214/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6561 - loss: 1.0149\n",
      "Epoch 81: val_loss did not improve from 0.84053\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6561 - loss: 1.0152 - val_accuracy: 0.7344 - val_loss: 0.8475\n",
      "Epoch 82/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6529 - loss: 1.0207\n",
      "Epoch 82: val_loss did not improve from 0.84053\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6530 - loss: 1.0207 - val_accuracy: 0.7390 - val_loss: 0.8658\n",
      "Epoch 83/100\n",
      "\u001b[1m200/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6479 - loss: 1.0209\n",
      "Epoch 83: val_loss improved from 0.84053 to 0.83273, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6477 - loss: 1.0216 - val_accuracy: 0.7298 - val_loss: 0.8327\n",
      "Epoch 84/100\n",
      "\u001b[1m192/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6629 - loss: 1.0192\n",
      "Epoch 84: val_loss improved from 0.83273 to 0.82361, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6629 - loss: 1.0171 - val_accuracy: 0.7470 - val_loss: 0.8236\n",
      "Epoch 85/100\n",
      "\u001b[1m192/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6609 - loss: 0.9985\n",
      "Epoch 85: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6610 - loss: 0.9995 - val_accuracy: 0.7367 - val_loss: 0.8455\n",
      "Epoch 86/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6530 - loss: 1.0237\n",
      "Epoch 86: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6531 - loss: 1.0234 - val_accuracy: 0.7247 - val_loss: 0.8608\n",
      "Epoch 87/100\n",
      "\u001b[1m217/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6710 - loss: 0.9999\n",
      "Epoch 87: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6709 - loss: 0.9999 - val_accuracy: 0.7270 - val_loss: 0.8406\n",
      "Epoch 88/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6641 - loss: 1.0001\n",
      "Epoch 88: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6641 - loss: 1.0000 - val_accuracy: 0.7298 - val_loss: 0.8537\n",
      "Epoch 89/100\n",
      "\u001b[1m209/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6642 - loss: 0.9919\n",
      "Epoch 89: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6646 - loss: 0.9909 - val_accuracy: 0.7207 - val_loss: 0.8309\n",
      "Epoch 90/100\n",
      "\u001b[1m188/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6633 - loss: 1.0335\n",
      "Epoch 90: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6638 - loss: 1.0291 - val_accuracy: 0.7396 - val_loss: 0.8610\n",
      "Epoch 91/100\n",
      "\u001b[1m211/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6708 - loss: 0.9787\n",
      "Epoch 91: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6706 - loss: 0.9796 - val_accuracy: 0.7378 - val_loss: 0.8314\n",
      "Epoch 92/100\n",
      "\u001b[1m216/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6601 - loss: 0.9924\n",
      "Epoch 92: val_loss did not improve from 0.82361\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6602 - loss: 0.9922 - val_accuracy: 0.7539 - val_loss: 0.8318\n",
      "Epoch 93/100\n",
      "\u001b[1m188/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6594 - loss: 1.0099\n",
      "Epoch 93: val_loss improved from 0.82361 to 0.82120, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6597 - loss: 1.0098 - val_accuracy: 0.7367 - val_loss: 0.8212\n",
      "Epoch 94/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6490 - loss: 1.0107\n",
      "Epoch 94: val_loss did not improve from 0.82120\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6490 - loss: 1.0107 - val_accuracy: 0.7401 - val_loss: 0.8360\n",
      "Epoch 95/100\n",
      "\u001b[1m200/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6620 - loss: 1.0087\n",
      "Epoch 95: val_loss did not improve from 0.82120\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6626 - loss: 1.0065 - val_accuracy: 0.7327 - val_loss: 0.8312\n",
      "Epoch 96/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6633 - loss: 1.0201\n",
      "Epoch 96: val_loss did not improve from 0.82120\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6633 - loss: 1.0201 - val_accuracy: 0.7418 - val_loss: 0.8244\n",
      "Epoch 97/100\n",
      "\u001b[1m188/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6675 - loss: 0.9799\n",
      "Epoch 97: val_loss improved from 0.82120 to 0.81393, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6677 - loss: 0.9803 - val_accuracy: 0.7487 - val_loss: 0.8139\n",
      "Epoch 98/100\n",
      "\u001b[1m190/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6675 - loss: 0.9812\n",
      "Epoch 98: val_loss improved from 0.81393 to 0.80093, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6672 - loss: 0.9833 - val_accuracy: 0.7481 - val_loss: 0.8009\n",
      "Epoch 99/100\n",
      "\u001b[1m210/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6648 - loss: 0.9902\n",
      "Epoch 99: val_loss did not improve from 0.80093\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6651 - loss: 0.9890 - val_accuracy: 0.7436 - val_loss: 0.8231\n",
      "Epoch 100/100\n",
      "\u001b[1m191/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6761 - loss: 0.9548\n",
      "Epoch 100: val_loss did not improve from 0.80093\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6755 - loss: 0.9575 - val_accuracy: 0.7167 - val_loss: 0.8285\n",
      "Training completed in time:  0:00:58.490500\n"
     ]
    }
   ],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.keras', verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7166571021080017\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-315.6028    ,   94.854805  ,  -37.22234   ,   46.778263  ,\n",
       "         -6.728693  ,   10.012548  ,   -1.607553  ,   18.511341  ,\n",
       "        -11.9006195 ,    7.5940356 ,   -7.8546596 ,   11.362425  ,\n",
       "        -15.617316  ,    3.301991  ,  -11.958161  ,    6.353489  ,\n",
       "         -5.5870256 ,   20.78539   ,   -0.4692282 ,    6.0436325 ,\n",
       "        -11.619548  ,    2.8686748 ,  -10.176432  ,    8.332485  ,\n",
       "          1.7765609 ,    2.5638974 ,  -14.761059  ,    1.1465564 ,\n",
       "          3.7835658 ,    3.1094651 ,  -12.185812  ,   -3.0522912 ,\n",
       "          3.7284145 ,    8.962753  ,    0.9306449 ,    3.1800797 ,\n",
       "          2.485049  ,    0.61386466,  -11.449189  ,   -6.0105853 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"bark.wav\"\n",
    "\n",
    "prediction_feature = features_extractor(filename)\n",
    "\n",
    "# model.predict(prediction_feature)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
